# MapReduce任务

MapReduce分为两个阶段，Map段和Reduce段。以一段字符串`hnu hnu`为例：

1. Map段：
   1. 会将字符串分割成`(hnu, 1)`、`(hnu, 1)`，然后传给Reduce阶段；
   2. Map段会接受两个输入，可以称作为是Keyin、Valuein，一般而言在Map阶段的输入Keyin是一个`LongWritable`的类型，Valuein是一个`Text`的类型，表示在所读取的文本中，这个`Text`相对于开头的偏移量为Keyin
   3. 它还会输出两个，可以称作为Keyout、Valueout，在程序中需要指定所输出的不同类别。
2. Reduce段：
   1. 会对同一组进行处理，也就是我们会得到`(hnu, (1, 1))`，因此可以知道第二个参数是可迭代的。
   2. 同样在Reduce段会接受Map阶段的两个输出，并将它们作为输入处理逻辑后输出。

在编写程序的时候，Hadoop要求还有一个Driver的入口程序，在里面要制定一些东西：

1. 获取Job任务的信息；
2. 设置jar包路径；
3. 关联mapper和reducer；
4. 设置mapper的输出kv类型；
5. 设置reducer的输出kv类型；
6. 设置整体文件的读入路径和写出路径，这个在本地Windows环境运行的时候，可以设置成自己电脑的路径，但是如果传到Hadoop集群上执行的话，那么推荐使用`arg[0]`以及`arg[1]`来动态接收文件路径。
7. 提交job。

**因此如果使用MapReduce进行业务处理的时候，通常包含Mapper、Reducer、Driver这三个类。**

## 1、数据去重

### 分析

我们可以发现，其实从Map段到Reduce段中会有自动帮我们完成去重的工作，因此只需指定：

1. Map的Keyout为`Text`、Valueout为`NullWritale`；
2. Reducer的Keyout为`Text`、Valueout为`NullWritable`

### Driver

```java
package Lin.Hnu.Summer.Task1.DataDe;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class DataDeDriver{
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 设置jar包路径
        job.setJarByClass(DataDeDriver.class);

        //3 关联mapper和reducer
        job.setMapperClass(DataDeMapper.class);
        job.setReducerClass(DataDeReducer.class);

        // 4 设置map的输出kv类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(NullWritable.class);

        // 5 设置最终输出的kv类型
        job.setOutputValueClass(Text.class);
        job.setOutputValueClass(NullWritable.class);

        // 6 设置输入路径以及输出路径
        FileInputFormat.setInputPaths(job, new Path("arg[0]"));
        FileOutputFormat.setOutputPath(job, new Path("agr[1]"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);

        System.exit(result ? 0 : 1);
    }
}

```

### Mapper

```java
package Lin.Hnu.Summer.Task1.DataDe;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/*
* Keyin: LongWritable 单词在文本中的偏移量
* Valuein: Text 文本
* Keyout: Text
* Valueout: IntWritable
* */

public class DataDeMapper extends Mapper<LongWritable, Text, Text, NullWritable> {
    private IntWritable outV = new IntWritable(1);
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        context.write(value, NullWritable.get()); // 直接写出就好了
    }
}
```

### Reducer

```java
package Lin.Hnu.Summer.Task1.DataDe;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;


/**
 * Keyin: map阶段的keyout  即 text
 * Valuein: map阶段的valueout 即IntWritable
 * Keyout: Text
 * Valueout: NullWritable
 */
public class DataDeReducer extends Reducer<Text, IntWritable, Text, NullWritable>{
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        context.write(key, NullWritable.get());
    }
}
```

### 测试数据

输入数据：

```
2012-3-1 a
2012-3-1 a
2012-3-1 a
2012-3-2 b
2012-3-2 b
2012-3-6 b
2012-3-6 b
2012-3-6 b
2012-3-1 a
2012-3-2 b
2012-3-6 b
2012-3-1 a
2012-3-1 a
2012-3-6 b
2012-3-1 a
2012-3-6 b
```

输出：

![image-20230830172123881](./实验报告/image-20230830172123881.png)

## 2、数据排序

### 分析

1. 我们应该知道的是mapreduce在map阶段会默认对输出的key进行排序，该排序是快排，如果传入的是字符串的话就是按照字典序排序了，因此我们应该传入整型。
2. 在reduce阶段对于每个相同的输入key都会调用一次reduce方法，所以只需要声明一个`cnt`表示编号，每次调用的时候动态+1即可。

### Driver

```java
package Lin.Hnu.Summer.Task2.Sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class SortDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 2 设置jar包路径
        job.setJarByClass(SortDriver.class);

        // 3 关联mapper和reducer
        job.setMapperClass(SortMapper.class);
        job.setReducerClass(SortReducer.class);

        // 4 设置mapper的kv输出
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(NullWritable.class);

        // 5 设置reducer的kv输出
        job.setOutputKeyClass(IntWritable.class);
        job.setOutputValueClass(IntWritable.class);

        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task2\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task2\\output"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### Mapper

```java
package Lin.Hnu.Summer.Task2.Sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class SortMapper extends Mapper<LongWritable, Text, IntWritable, NullWritable> {
    private IntWritable outV = new IntWritable(1);

    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, IntWritable, NullWritable>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        outV.set(Integer.parseInt(line));
        context.write(outV, NullWritable.get());
    }
}

```

### Reducer

```java
package Lin.Hnu.Summer.Task2.Sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class SortReducer extends Reducer<IntWritable, NullWritable, IntWritable, IntWritable> {
    private IntWritable outK = new IntWritable();
    private Integer cnt = 1;

    @Override
    protected void reduce(IntWritable key, Iterable<NullWritable> values, Reducer<IntWritable, NullWritable, IntWritable, IntWritable>.Context context) throws IOException, InterruptedException {
        outK.set(cnt);
        cnt ++;
        context.write(outK, key);
    }
}
```

### 测试数据

![image-20230831161409453](./实验报告/image-20230831161409453.png)

运行的输出：

![image-20230831161451077](./实验报告/image-20230831161451077.png)

## 3、平均成绩

### 分析

由于map阶段会将每一行处理成对应的键值对，所以在reduce阶段进行输出值的综合即可。

### Driver

```java
package Lin.Hnu.Summer.Task3.AvgGrade;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class AvgGradeDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 2 设置jar包路径
        job.setJarByClass(AvgGradeDriver.class);

        // 3 关联 mapper 和 reducer
        job.setMapperClass(AvgGradeMapper.class);
        job.setReducerClass(AvgGradeReducer.class);

        // 4 设置mapper输出kv
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // 5 设置 reducer输出kv
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);

        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task3\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task3\\output"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### Mapper

```java
package Lin.Hnu.Summer.Task3.AvgGrade;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Keyin: 偏移量
 * Valuein: 文本的一行
 * Keyout: 姓名
 * Valueout: 当前人的成绩
 */
public class AvgGradeMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private Text outK = new Text();
    private IntWritable outV = new IntWritable();
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split("    ");

        outK.set(words[0]);
        outV.set(Integer.parseInt(words[1]));
        context.write(outK, outV);
    }
}
```

### Reducer

```java
package Lin.Hnu.Summer.Task3.AvgGrade;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Keyin: 偏移量
 * Valuein: 文本的一行
 * Keyout: 姓名
 * Valueout: 当前人的成绩
 */
public class AvgGradeMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private Text outK = new Text();
    private IntWritable outV = new IntWritable();
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split("    ");

        outK.set(words[0]);
        outV.set(Integer.parseInt(words[1]));
        context.write(outK, outV);
    }
}
```

### 测试数据

输入：

![image-20230831171334026](./实验报告/image-20230831171334026.png)

输出：

![image-20230831171412198](./实验报告/image-20230831171412198.png)

## 4、单表连接

### 分析

1. 给一个数据表项代表父子关系，请我们找出所有的爷孙关系。我的思路是以某一位父亲作为中间键，通过它既能找到他的儿子，也能找到他的父亲。

2. 由于map阶段后，Hadoop底层将所有相同的keyout值对应的valueout组合成一个迭代器传送给reduce阶段，所以某一位父亲就作为这里的keyout，而它对应的儿子与父亲则作为valueout，我们在map阶段会输出2次。

3. 为了方便在reduce阶段辨别传入的键值对是父-子关系还是子-父关系，人为在对应的value后面添加一个标志位，0代表父-子关系，1代表子-父关系。
4. 在reduce阶段利用两个`ArrayList` `grandsons`和`grandparents`收集对应的信息，如果标志位为0，那么代表此时的value是当前key的儿子，因此插入grandsons；如果是1，则value是当前key的父亲，因此插入grandparents。
5. 最后嵌套遍历，先遍历`grandsons`再遍历`grandparents`即可找出所有的爷孙关系。

---

这么说比较抽象，举一个例子：

比如我们有父子关系（左子右父）：`<Tom, Jack>` `<Jack, Alice>`

1. 那么在map阶段我们应该输出，`<Jack, "Tom 0">` `<Jack, "Alice 1">` `<Tom, "Jack 1">` `<Alice, "Jack 0">`；

2. 经过map阶段，在底层整合成：`<Jack, ["Tom 0", "Alice 1"]>` `<Tom, ["Jack 1"]>` `<Alice, ["Jack 0"]>`传给reduce阶段；
3. 在reduce阶段，有效的一组信息是`grandsons = [Tom]` `grandparents = [Alice]` 所以最后输出爷孙关系（左孙右爷）`<Tom, Alice>`

### Driver

```java
package Lin.Hnu.Summer.Task4.Relation;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class RelationDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 2 设置jar包路径
        job.setJarByClass(RelationDriver.class);

        // 3 关联mapper以及reducer
        job.setMapperClass(RelationMapper.class);
        job.setReducerClass(RelationReducer.class);

        // 4 设置mapper输出格式
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        // 5 设置reducer输出格式
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task4\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task4\\output"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### Mapper

```java
package Lin.Hnu.Summer.Task4.Relation;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Keyout: 父亲的名字                         | 儿子的名字
 * Valueout: 儿子的名字 + '0' 表示是父子关系    | 父亲的名字 + '1'
 */
public class RelationMapper extends Mapper<LongWritable, Text, Text, Text> {
    private Text outV = new Text();
    private Text outK = new Text();
    private String tmpv;
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split(" ");
        // Tom Jack
        tmpv = String.format("%s %d", words[1], 1); // 表示是子父关系 <Tom, "Jack 1">
        outK.set(words[0]);
        outV.set(tmpv);
        context.write(outK, outV);
        outK.set(words[1]);
        tmpv = String.format("%s %d", words[0], 0); // 表示是父子关系 <Jack, "Tom 0">
        outV.set(tmpv);
        context.write(outK, outV);
    }
}

```

### Reducer

```java
package Lin.Hnu.Summer.Task4.Relation;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.ArrayList;

public class RelationReducer extends Reducer<Text, Text, Text, Text> {
    private String MapoutV;
    private Text outK = new Text();
    private Text outV = new Text();
    @Override
    protected void reduce(Text key, Iterable<Text> values, Reducer<Text, Text, Text, Text>.Context context) throws IOException, InterruptedException {
        ArrayList<String> grandparents = new ArrayList<>();
        ArrayList<String> grandsons = new ArrayList<>();
        for (Text value : values) {
            MapoutV = value.toString(); // ["Tom 0", "Alice 1"]
            String[] words = MapoutV.split(" ");
            String t1 = words[0], t2 = words[1];
            if(t2.equals("1")) { // 表明是子父关系 MapoutK 是儿子 MapoutV是父亲 Alice是爷爷
                grandparents.add(t1);
            }else { // 表明是父子关系 MapoutK是父亲 MapoutV是儿子 Tom是孙子
                grandsons.add(t1);
            }
        }
        for (String grandson : grandsons) {
            for (String grandparent : grandparents) {
                outK.set(grandson);
                outV.set(grandparent);
                context.write(outK, outV);
            }
        }
    }
}
```

### 测试数据

输入：

![image-20230901152305284](./实验报告/image-20230901152305284.png)

输出：

![image-20230901152356418](./实验报告/image-20230901152356418.png)

# 出现的问题

## 1. ens33网卡消失

ens33网卡是配置ip与端口的网卡，非常重要，没有他集群就需要重新配置，今天登录的时候意外发现有一个机器通过xshell连接不上。

检查发现是ens33这个网卡不见了。

![image-20230904152219899](./实验报告/image-20230904152219899.png)

通过重启网络相关服务成功找到了ens33这个网卡并连接上了此机器。

![image-20230904152747602](./实验报告/image-20230904152747602.png)

命令如下：

```sh
systemctl stop NetworkManager
systemctl restart network.service
service network restart
```

