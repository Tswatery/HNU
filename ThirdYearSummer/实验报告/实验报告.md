# 一、MapReduce任务

MapReduce分为两个阶段，Map段和Reduce段。以一段字符串`hnu hnu`为例：

1. Map段：
   1. 会将字符串分割成`(hnu, 1)`、`(hnu, 1)`，然后传给Reduce阶段；
   2. Map段会接受两个输入，可以称作为是Keyin、Valuein，一般而言在Map阶段的输入Keyin是一个`LongWritable`的类型，Valuein是一个`Text`的类型，表示在所读取的文本中，这个`Text`相对于开头的偏移量为Keyin
   3. 它还会输出两个，可以称作为Keyout、Valueout，在程序中需要指定所输出的不同类别。
2. Reduce段：
   1. 会对同一组进行处理，也就是我们会得到`(hnu, (1, 1))`，因此可以知道第二个参数是可迭代的。
   2. 同样在Reduce段会接受Map阶段的两个输出，并将它们作为输入处理逻辑后输出。

在编写程序的时候，Hadoop要求还有一个Driver的入口程序，在里面要制定一些东西：

1. 获取Job任务的信息；
2. 设置jar包路径；
3. 关联mapper和reducer；
4. 设置mapper的输出kv类型；
5. 设置reducer的输出kv类型；
6. 设置整体文件的读入路径和写出路径，这个在本地Windows环境运行的时候，可以设置成自己电脑的路径，但是如果传到Hadoop集群上执行的话，那么推荐使用`arg[0]`以及`arg[1]`来动态接收文件路径。
7. 提交job。

**因此如果使用MapReduce进行业务处理的时候，通常包含Mapper、Reducer、Driver这三个类。**

## 1、数据去重

### 分析

我们可以发现，其实从Map段到Reduce段中会有自动帮我们完成去重的工作，因此只需指定：

1. Map的Keyout为`Text`、Valueout为`NullWritale`；
2. Reducer的Keyout为`Text`、Valueout为`NullWritable`

### Driver

```java
package Lin.Hnu.Summer.Task1.DataDe;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class DataDeDriver{
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 设置jar包路径
        job.setJarByClass(DataDeDriver.class);

        //3 关联mapper和reducer
        job.setMapperClass(DataDeMapper.class);
        job.setReducerClass(DataDeReducer.class);

        // 4 设置map的输出kv类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(NullWritable.class);

        // 5 设置最终输出的kv类型
        job.setOutputValueClass(Text.class);
        job.setOutputValueClass(NullWritable.class);

        // 6 设置输入路径以及输出路径
        FileInputFormat.setInputPaths(job, new Path("arg[0]"));
        FileOutputFormat.setOutputPath(job, new Path("agr[1]"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);

        System.exit(result ? 0 : 1);
    }
}

```

### Mapper

```java
package Lin.Hnu.Summer.Task1.DataDe;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/*
* Keyin: LongWritable 单词在文本中的偏移量
* Valuein: Text 文本
* Keyout: Text
* Valueout: IntWritable
* */

public class DataDeMapper extends Mapper<LongWritable, Text, Text, NullWritable> {
    private IntWritable outV = new IntWritable(1);
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        context.write(value, NullWritable.get()); // 直接写出就好了
    }
}
```

### Reducer

```java
package Lin.Hnu.Summer.Task1.DataDe;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;


/**
 * Keyin: map阶段的keyout  即 text
 * Valuein: map阶段的valueout 即IntWritable
 * Keyout: Text
 * Valueout: NullWritable
 */
public class DataDeReducer extends Reducer<Text, IntWritable, Text, NullWritable>{
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Reducer<Text, IntWritable, Text, NullWritable>.Context context) throws IOException, InterruptedException {
        context.write(key, NullWritable.get());
    }
}
```

### 测试数据

输入数据：

```
2012-3-1 a
2012-3-1 a
2012-3-1 a
2012-3-2 b
2012-3-2 b
2012-3-6 b
2012-3-6 b
2012-3-6 b
2012-3-1 a
2012-3-2 b
2012-3-6 b
2012-3-1 a
2012-3-1 a
2012-3-6 b
2012-3-1 a
2012-3-6 b
```

输出：

![image-20230830172123881](./实验报告/image-20230830172123881.png)

## 2、数据排序

### 分析

1. 我们应该知道的是mapreduce在map阶段会默认对输出的key进行排序，该排序是快排，如果传入的是字符串的话就是按照字典序排序了，因此我们应该传入整型。
2. 在reduce阶段对于每个相同的输入key都会调用一次reduce方法，所以只需要声明一个`cnt`表示编号，每次调用的时候动态+1即可。

### Driver

```java
package Lin.Hnu.Summer.Task2.Sort;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class SortDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 2 设置jar包路径
        job.setJarByClass(SortDriver.class);

        // 3 关联mapper和reducer
        job.setMapperClass(SortMapper.class);
        job.setReducerClass(SortReducer.class);

        // 4 设置mapper的kv输出
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(NullWritable.class);

        // 5 设置reducer的kv输出
        job.setOutputKeyClass(IntWritable.class);
        job.setOutputValueClass(IntWritable.class);

        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task2\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task2\\output"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### Mapper

```java
package Lin.Hnu.Summer.Task2.Sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

public class SortMapper extends Mapper<LongWritable, Text, IntWritable, NullWritable> {
    private IntWritable outV = new IntWritable(1);

    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, IntWritable, NullWritable>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        outV.set(Integer.parseInt(line));
        context.write(outV, NullWritable.get());
    }
}

```

### Reducer

```java
package Lin.Hnu.Summer.Task2.Sort;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class SortReducer extends Reducer<IntWritable, NullWritable, IntWritable, IntWritable> {
    private IntWritable outK = new IntWritable();
    private Integer cnt = 1;

    @Override
    protected void reduce(IntWritable key, Iterable<NullWritable> values, Reducer<IntWritable, NullWritable, IntWritable, IntWritable>.Context context) throws IOException, InterruptedException {
        outK.set(cnt);
        cnt ++;
        context.write(outK, key);
    }
}
```

### 测试数据

![image-20230831161409453](./实验报告/image-20230831161409453.png)

运行的输出：

![image-20230831161451077](./实验报告/image-20230831161451077.png)

## 3、平均成绩

### 分析

由于map阶段会将每一行处理成对应的键值对，所以在reduce阶段进行输出值的综合即可。

### Driver

```java
package Lin.Hnu.Summer.Task3.AvgGrade;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class AvgGradeDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 2 设置jar包路径
        job.setJarByClass(AvgGradeDriver.class);

        // 3 关联 mapper 和 reducer
        job.setMapperClass(AvgGradeMapper.class);
        job.setReducerClass(AvgGradeReducer.class);

        // 4 设置mapper输出kv
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // 5 设置 reducer输出kv
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);

        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task3\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task3\\output"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### Mapper

```java
package Lin.Hnu.Summer.Task3.AvgGrade;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Keyin: 偏移量
 * Valuein: 文本的一行
 * Keyout: 姓名
 * Valueout: 当前人的成绩
 */
public class AvgGradeMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private Text outK = new Text();
    private IntWritable outV = new IntWritable();
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split("    ");

        outK.set(words[0]);
        outV.set(Integer.parseInt(words[1]));
        context.write(outK, outV);
    }
}
```

### Reducer

```java
package Lin.Hnu.Summer.Task3.AvgGrade;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Keyin: 偏移量
 * Valuein: 文本的一行
 * Keyout: 姓名
 * Valueout: 当前人的成绩
 */
public class AvgGradeMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
    private Text outK = new Text();
    private IntWritable outV = new IntWritable();
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split("    ");

        outK.set(words[0]);
        outV.set(Integer.parseInt(words[1]));
        context.write(outK, outV);
    }
}
```

### 测试数据

输入：

![image-20230831171334026](./实验报告/image-20230831171334026.png)

输出：

![image-20230831171412198](./实验报告/image-20230831171412198.png)

## 4、单表连接

### 分析

1. 给一个数据表项代表父子关系，请我们找出所有的爷孙关系。我的思路是以某一位父亲作为中间键，通过它既能找到他的儿子，也能找到他的父亲。

2. 由于map阶段后，Hadoop底层将所有相同的keyout值对应的valueout组合成一个迭代器传送给reduce阶段，所以某一位父亲就作为这里的keyout，而它对应的儿子与父亲则作为valueout，我们在map阶段会输出2次。

3. 为了方便在reduce阶段辨别传入的键值对是父-子关系还是子-父关系，人为在对应的value后面添加一个标志位，0代表父-子关系，1代表子-父关系。
4. 在reduce阶段利用两个`ArrayList` `grandsons`和`grandparents`收集对应的信息，如果标志位为0，那么代表此时的value是当前key的儿子，因此插入grandsons；如果是1，则value是当前key的父亲，因此插入grandparents。
5. 最后嵌套遍历，先遍历`grandsons`再遍历`grandparents`即可找出所有的爷孙关系。

---

这么说比较抽象，举一个例子：

比如我们有父子关系（左子右父）：`<Tom, Jack>` `<Jack, Alice>`

1. 那么在map阶段我们应该输出，`<Jack, "Tom 0">` `<Jack, "Alice 1">` `<Tom, "Jack 1">` `<Alice, "Jack 0">`；

2. 经过map阶段，在底层整合成：`<Jack, ["Tom 0", "Alice 1"]>` `<Tom, ["Jack 1"]>` `<Alice, ["Jack 0"]>`传给reduce阶段；
3. 在reduce阶段，有效的一组信息是`grandsons = [Tom]` `grandparents = [Alice]` 所以最后输出爷孙关系（左孙右爷）`<Tom, Alice>`

### Driver

```java
package Lin.Hnu.Summer.Task4.Relation;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class RelationDriver {
    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
        // 1 获取job
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf);

        // 2 设置jar包路径
        job.setJarByClass(RelationDriver.class);

        // 3 关联mapper以及reducer
        job.setMapperClass(RelationMapper.class);
        job.setReducerClass(RelationReducer.class);

        // 4 设置mapper输出格式
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);

        // 5 设置reducer输出格式
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);

        // 6 设置输入输出路径
        FileInputFormat.setInputPaths(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task4\\input"));
        FileOutputFormat.setOutputPath(job, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task4\\output"));

        // 7 提交job
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }
}
```

### Mapper

```java
package Lin.Hnu.Summer.Task4.Relation;

import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Keyout: 父亲的名字                         | 儿子的名字
 * Valueout: 儿子的名字 + '0' 表示是父子关系    | 父亲的名字 + '1'
 */
public class RelationMapper extends Mapper<LongWritable, Text, Text, Text> {
    private Text outV = new Text();
    private Text outK = new Text();
    private String tmpv;
    @Override
    protected void map(LongWritable key, Text value, Mapper<LongWritable, Text, Text, Text>.Context context) throws IOException, InterruptedException {
        String line = value.toString();
        String[] words = line.split(" ");
        // Tom Jack
        tmpv = String.format("%s %d", words[1], 1); // 表示是子父关系 <Tom, "Jack 1">
        outK.set(words[0]);
        outV.set(tmpv);
        context.write(outK, outV);
        outK.set(words[1]);
        tmpv = String.format("%s %d", words[0], 0); // 表示是父子关系 <Jack, "Tom 0">
        outV.set(tmpv);
        context.write(outK, outV);
    }
}

```

### Reducer

```java
package Lin.Hnu.Summer.Task4.Relation;

import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;
import java.util.ArrayList;

public class RelationReducer extends Reducer<Text, Text, Text, Text> {
    private String MapoutV;
    private Text outK = new Text();
    private Text outV = new Text();
    @Override
    protected void reduce(Text key, Iterable<Text> values, Reducer<Text, Text, Text, Text>.Context context) throws IOException, InterruptedException {
        ArrayList<String> grandparents = new ArrayList<>();
        ArrayList<String> grandsons = new ArrayList<>();
        for (Text value : values) {
            MapoutV = value.toString(); // ["Tom 0", "Alice 1"]
            String[] words = MapoutV.split(" ");
            String t1 = words[0], t2 = words[1];
            if(t2.equals("1")) { // 表明是子父关系 MapoutK 是儿子 MapoutV是父亲 Alice是爷爷
                grandparents.add(t1);
            }else { // 表明是父子关系 MapoutK是父亲 MapoutV是儿子 Tom是孙子
                grandsons.add(t1);
            }
        }
        for (String grandson : grandsons) {
            for (String grandparent : grandparents) {
                outK.set(grandson);
                outV.set(grandparent);
                context.write(outK, outV);
            }
        }
    }
}
```

### 测试数据

输入：

![image-20230901152305284](./实验报告/image-20230901152305284.png)

输出：

![image-20230901152356418](./实验报告/image-20230901152356418.png)

# 二、使用javaAPI操作hdfs

Hdfs是指Hadoop分布式文件系统，使用将超大文件分成小文件来存的思想。这个任务比较简单，不用像mapreduce一样来写三个类。

## 为什么要使用javaAPI来操作？

hdfs的操作无非涉及传统Linux中的`mkdir`、`rm`、`put(上传)`等操作，一般只需要一行命令就可以了，为什么还要使用繁琐的javaAPI来操作呢？

这是因为我们之前是通过xshell远程连接的Hadoop集群,但是xshell毕竟是一种终端,手动输入并不是非常方便,我们希望在Windows上创建一个hdfs的客户端直接调用某些API即可完成操作.

这个客户端的大致思路是先连接上Hadoop的namenode节点,这是因为它负责接收客户端的请求.

客户端的思路大致如下:

1. 获取一个客户端对象;
2. 执行相关命令;
3. 关闭资源即可.

![image-20230905092558847](./实验报告/image-20230905092558847.png)

## 具体实现细节

```java
@Test
public void FileUpload() throws URISyntaxException, IOException, InterruptedException {
    init();
    /**
     * @param1: Boolean Whether to delete source file
     * @param2: Boolean Whether to overwrite destination file if already exists
     * @param3: Path of source file
     * @param4: Path of destination file
     */
    fs.copyFromLocalFile(false, false, new Path("D:\\IdeaProject\\MapReduce1\\input\\Task1\\Input\\input1"), new Path( "/"));
    close();
}
```

由于每个函数都涉及获取客户端对象和一些初始化信息,所以将它们单独封装起来.

```java
@Before
public void init() throws URISyntaxException, IOException, InterruptedException {
    // 连接集群的namenode地址
    URI uri = new URI("hdfs://hadoop102:8020");// hadoop102内部的通信端口是8020
    // 创建一个配置文件
    Configuration configuration = new Configuration();

    // 指定用户
    String usr = "lin";

    // 1 拿到客户端对象
    fs = FileSystem.get(uri, configuration, usr);
}
@After
public void close() throws IOException {
    fs.close();
}
```

这样就能方便的测试了.

## 测试

### 1. 创建文件夹以及删除文件夹

```java
@Test
public void Filemkdir() throws URISyntaxException, IOException, InterruptedException {
    //获取资源
    init();
    // 2 创建一个文件夹
    fs.mkdirs(new Path("/testmkdir"));
    // 3 关闭资源
    close();
}

@Test
public void FileRmdir() throws URISyntaxException, IOException, InterruptedException {
    init();
    /**
     * @param1: file path
     * @param1: Whether to delete directory recursively
     */
    fs.delete(new Path("/testmkdir"), true);
    close();
}
```

直接点击这个就可以了,不过要保证Hadoop集群是打开的.

![image-20230905102129036](./实验报告/image-20230905102129036.png)

![](./实验报告/mkdir_rmdir.gif)

## 2. 测试读取文件

```java
@Test
public void FileDetail() throws IOException, URISyntaxException, InterruptedException {
    init();
    /**
     * @param1: path of file
     * @param2: Whether recursively
     */
    RemoteIterator<LocatedFileStatus> files = fs.listFiles(new Path("/"), true);

    // 遍历
    while (files.hasNext()) {
        LocatedFileStatus fileStatus = files.next();
        if(fileStatus.isFile()){
            Path path = fileStatus.getPath();
            System.out.printf("-------File name is [%s]--------\n", fileStatus.getPath().getName());
            fs.open(path);
            FSDataInputStream in = fs.open(path);
            BufferedReader d = new BufferedReader(new InputStreamReader(in));
            String content;
            while((content = d.readLine()) != null) {
                System.out.println(content);
            }
            d.close();
            in.close();
        }
    }
    close();
}
```



![image-20230905102914722](./实验报告/image-20230905102914722.png)

读取文件我是将其输出到stdout的,并且由于Hadoop有tmp文件夹中有大量系统信息,也会被一并输出这会导致输出并不是那么美观,目前正在尝试寻找办法解决不读取tmp文件夹但是仍读取其他文件夹的办法.

# 三、hbase





# 出现的问题

## 1. ens33网卡消失

ens33网卡是配置ip与端口的网卡，非常重要，没有他集群就需要重新配置，某天登录的时候意外发现有一个机器通过xshell连接不上。

检查发现是ens33这个网卡不见了。

![image-20230904152219899](./实验报告/image-20230904152219899.png)

通过重启网络相关服务成功找到了ens33这个网卡并连接上了此机器。

![image-20230904152747602](./实验报告/image-20230904152747602.png)

命令如下：

```sh
systemctl stop NetworkManager
systemctl restart network.service
service network restart
```

